{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/01 11:18:20 WARN Utils: Your hostname, kg3597wc201 resolves to a loopback address: 127.0.1.1; using 10.19.82.77 instead (on interface wifi0)\n",
      "22/11/01 11:18:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/01 11:18:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"count app\")\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - Time between earthquakes\n",
    "\n",
    "Suppose that earthquakes of a certain magnitude in a specific region can be modeled as a Poisson process with a mean of $\\lambda = 4.5$ earthquakes per day.  Let $X$ be the time until the third earth quake.  It can be shown that $X$ has a $Gamma$ distribution with $\\alpha = 3$ (number of events) and $\\beta = \\frac{1}{\\lambda}=\\frac{1}{4.5}$ (average time until the 3rd earthquake).  We can use Python's `random.gammavariate` to simulate the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method gammavariate in module random:\n",
      "\n",
      "gammavariate(alpha, beta) method of random.Random instance\n",
      "    Gamma distribution.  Not the gamma function!\n",
      "    \n",
      "    Conditions on the parameters are alpha > 0 and beta > 0.\n",
      "    \n",
      "    The probability distribution function is:\n",
      "    \n",
      "                x ** (alpha - 1) * math.exp(-x / beta)\n",
      "      pdf(x) =  --------------------------------------\n",
      "                  math.gamma(alpha) * beta ** alpha\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import gammavariate\n",
    "help(gammavariate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4998853733082485,\n",
       " 0.13343252481057666,\n",
       " 0.47471498369050863,\n",
       " 0.35024326794863625,\n",
       " 1.1036031572902247]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from composable.sequence import head\n",
    "N = 1000000\n",
    "time_between_3_quakes = [gammavariate(3,1/4.5) for i in range(N)]\n",
    "time_between_3_quakes >> head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three `for` loop patterns\n",
    "\n",
    "Most all `for` loops are reinventing one of the following patterns.\n",
    "\n",
    "1. **Map**ping a function/transformation unto each value.\n",
    "2. **Filter**ing the values by some boolean condition.\n",
    "3. **Reduce** values to one or more statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map example - Convert the times from days to hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11.997248959397965,\n",
       " 3.20238059545384,\n",
       " 11.393159608572207,\n",
       " 8.40583843076727,\n",
       " 26.486475774965392]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loop solution\n",
    "time_in_hours = []\n",
    "for t in time_between_3_quakes:\n",
    "    time_in_hours.append(t*24)\n",
    "time_in_hours >> head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11.997248959397965,\n",
       " 3.20238059545384,\n",
       " 11.393159608572207,\n",
       " 8.40583843076727,\n",
       " 26.486475774965392]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comprehension solution\n",
    "time_in_hours = [t*24 for t in time_between_3_quakes]\n",
    "time_in_hours >> head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<itertools.islice at 0x7f9fd1ed4130>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# better built-in python way\n",
    "time_in_hours = map(lambda t: t*24, time_between_3_quakes) >> head(5)\n",
    "time_in_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11.997248959397965,\n",
       " 3.20238059545384,\n",
       " 11.393159608572207,\n",
       " 8.40583843076727,\n",
       " 26.486475774965392]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in time_in_hours]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11.997248959397965,\n",
       " 3.20238059545384,\n",
       " 11.393159608572207,\n",
       " 8.40583843076727,\n",
       " 26.486475774965392]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With pipeable functions\n",
    "#from composable.strict import map\n",
    "from composable import strict as cs\n",
    "\n",
    "(time_between_3_quakes\n",
    " >> cs.map(lambda t: t*24)\n",
    " >> head(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Example -  filter out all value less than 1 day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4998853733082485,\n",
       " 0.13343252481057666,\n",
       " 0.47471498369050863,\n",
       " 0.35024326794863625,\n",
       " 0.49701079206183535]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop solution\n",
    "less_than_1_day = []\n",
    "for t in time_between_3_quakes:\n",
    "    if t < 1:\n",
    "        less_than_1_day.append(t)\n",
    "less_than_1_day >> head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4998853733082485,\n",
       " 0.13343252481057666,\n",
       " 0.47471498369050863,\n",
       " 0.35024326794863625,\n",
       " 0.49701079206183535]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comprehension solution\n",
    "less_than_1_day = [t for t in time_between_3_quakes if t < 1]\n",
    "less_than_1_day >> head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter() is also built into python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4998853733082485,\n",
       " 0.13343252481057666,\n",
       " 0.47471498369050863,\n",
       " 0.35024326794863625,\n",
       " 0.49701079206183535]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeable functions\n",
    "#from composable.strict import filter\n",
    "\n",
    "(time_between_3_quakes\n",
    " >> cs.filter(lambda t: t < 1)\n",
    " >> head(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reduce Example - Accumulating the maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.264879900932709"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loop solution\n",
    "max_time = 0 # safe since Gamma is non-negative\n",
    "for t in time_between_3_quakes:\n",
    "    max_time = max(max_time, t) # update step\n",
    "max_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.264879900932709"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Functional solution\n",
    "from functools import reduce\n",
    "\n",
    "reduce(lambda m, t: max(m, t), time_between_3_quakes, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function reduce in module _functools:\n",
      "\n",
      "reduce(...)\n",
      "    reduce(function, iterable[, initial]) -> value\n",
      "    \n",
      "    Apply a function of two arguments cumulatively to the items of a sequence\n",
      "    or iterable, from left to right, so as to reduce the iterable to a single\n",
      "    value.  For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates\n",
      "    ((((1+2)+3)+4)+5).  If initial is present, it is placed before the items\n",
      "    of the iterable in the calculation, and serves as a default when the\n",
      "    iterable is empty.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeable solution\n",
    "from composable import pipeable\n",
    "\n",
    "update_max = lambda m, t: max(m, t)\n",
    "\n",
    "@pipeable\n",
    "def my_reduce(func, xs, init = None):\n",
    "    if init is None:\n",
    "        return reduce(func, xs) # Uses first value as init\n",
    "    else:\n",
    "        return reduce(func, xs, init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.264879900932709"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with init = 0\n",
    "(time_between_3_quakes\n",
    " >> my_reduce(update_max, init = 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.264879900932709"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with init = first value\n",
    "(time_between_3_quakes\n",
    " >> my_reduce(update_max)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So Iverson, why are you such a piping fanboi!?!??\n",
    "\n",
    "Legos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "948"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the number of time less than 1 hour\n",
    "(time_between_3_quakes\n",
    " >> cs.map(lambda t: 24*t)\n",
    " >> cs.filter(lambda t: t < 1)\n",
    " >> my_reduce(lambda cnt, t: cnt + 1, init = 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\"> Task 6.1.3 </font>\n",
    "\n",
    "Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reusable, generic components that are immediately compatible with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So ... about those loops ...\n",
    "\n",
    "<img src=\"./img/no_more_for_loops.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loops don't work well on multiple or multi-core machines\n",
    "\n",
    "<img src=\"./img/loop_problems.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about functions?\n",
    "\n",
    "* Using [lambda calculus](https://en.wikipedia.org/wiki/Lambda_calculus) we can show that all functional programs that terminate will provide the same result regardless of the order of execution.\n",
    "* This explains why `pyspark` uses functional idioms like `map`, `filter`, and `reduce`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  The `pyspark.RDD` \n",
    "\n",
    "> A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable, partitioned  collection of elements that can be operated on in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_RDD = sc.parallelize(time_between_3_quakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "948"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the number of time less than 1 hour\n",
    "(times_RDD\n",
    " .map(lambda t: 24*t)\n",
    " .filter(lambda t: t < 1)\n",
    " .map(lambda t: 1)\n",
    " .cache()\n",
    " .reduce(lambda cnt, t: cnt + 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method cache in module pyspark.rdd:\n",
      "\n",
      "cache() -> 'RDD[T]' method of pyspark.rdd.RDD instance\n",
      "    Persist this RDD with the default storage level (`MEMORY_ONLY`).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(times_RDD.cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method reduce in module pyspark.rdd:\n",
      "\n",
      "reduce(f: Callable[[~T, ~T], ~T]) -> ~T method of pyspark.rdd.RDD instance\n",
      "    Reduces the elements of this RDD using the specified commutative and\n",
      "    associative binary operator. Currently reduces partitions locally.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from operator import add\n",
      "    >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n",
      "    15\n",
      "    >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)\n",
      "    10\n",
      "    >>> sc.parallelize([]).reduce(add)\n",
      "    Traceback (most recent call last):\n",
      "        ...\n",
      "    ValueError: Can not reduce() empty RDD\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(times_RDD.reduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color=\"red\"><h2> Task 6.1.2 </h2></font>\n",
    "\n",
    "Use our three functional idioms to compute the average time (in seconds) of all times greater than 1 hour, in two ways.\n",
    "\n",
    "1. In python using the various `pipeable` functions presented earlier.\n",
    "2. Using `pyspark` RDD's\n",
    "\n",
    "**Hint 1.** Computing the mean requires that we (A) compute both the total and count, then (B) divide.\n",
    "\n",
    "**Hint 2.** Allow yourself two passes through the data for 2. and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57707.45826491965"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeable function solution\n",
    "from operator import add\n",
    "\n",
    "filtered_time_seconds = (time_between_3_quakes\n",
    "    >> cs.filter(lambda t: t*24 > 1)\n",
    "    >> cs.map(lambda t: t*24*3600)    \n",
    ")\n",
    "\n",
    "total = filtered_time_seconds >> my_reduce(add)\n",
    "count = filtered_time_seconds >> my_reduce(lambda x,y: x+1, init=0)\n",
    "total/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57707.45826491965"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total/len(filtered_time_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57707.45826491965"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(filtered_time_seconds)/len(filtered_time_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57707.45826491965"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pyspark RDD solution\n",
    "\n",
    "filtered_RDD = (times_RDD\n",
    "    .filter(lambda t: t*24 > 1)\n",
    "    .map(lambda t: t*24*3600)\n",
    ") \n",
    "\n",
    "rdd_total = filtered_RDD.reduce(add)\n",
    "rdd_count = filtered_RDD.map(lambda x: 1).reduce(add)\n",
    "rdd_total/rdd_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57707.45826491965"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_total / filtered_RDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color=\"red\"><h2> Task 6.1.3 </h2></font>\n",
    "\n",
    "The variance of a random variable is the average square of the difference between $X$ and ~~it's~~ its mean.  Use the three functional idioms to compute the variance of the times in three ways:\n",
    "\n",
    "1. In python using a `for` loop.\n",
    "2. In python using the various `pipeable` functions presented earlier.\n",
    "3. Using `pyspark` RDD's\n",
    "\n",
    "**Hint 1.** It can be shown that the mean of our distribution is $\\alpha*\\beta = \\frac{3}{4.5}$.\n",
    "\n",
    "**Hint 2.** Subtract, then square, then average.\n",
    "\n",
    "**Hint 3.** In this case, we can show $V(X) = \\alpha\\beta^2 = \\frac{3}{4.5^2}$.  Use this to check your approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14848240995529485"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best solution\n",
    "from statistics import variance\n",
    "variance(time_between_3_quakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "MU = 3/4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14848267075058352"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for loop solution\n",
    "total = 0\n",
    "count = 0\n",
    "\n",
    "for t in time_between_3_quakes:\n",
    "    total += (t-MU)**2\n",
    "    count += 1\n",
    "\n",
    "total/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14814814814814814"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 / 4.5**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14848267075058352"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeable function solution\n",
    "standardized = (time_between_3_quakes\n",
    "    >> cs.map(lambda t: (t-MU)**2)\n",
    ")\n",
    "\n",
    "total = standardized >> my_reduce(add)\n",
    "count = standardized >> my_reduce(lambda x,y: x+1, init=0)\n",
    "total/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14848267075058352"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pyspark RDD solution\n",
    "standardized_RDD = (times_RDD\n",
    "    .map(lambda t: (t-MU)**2)\n",
    ") \n",
    "\n",
    "rdd_total = standardized_RDD.reduce(add)\n",
    "rdd_count = standardized_RDD.map(lambda x: 1).reduce(add)\n",
    "rdd_total/rdd_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
